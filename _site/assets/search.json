

[
  
  
    
    
      {
        "title": "Description of an Alembic",
        "excerpt": "The complete distilling apparatus consists of three parts: the “cucurbit” (Arabic ḳarʿa, Greek βίκος), the still pot containing the liquid to be distilled, which is heated by a flame; the “head” or “cap” (Arabic anbiḳ, Greek ἄμβιξ) which fits over the mouth of the cucurbit to receive the vapors, with an attached downward-sloping “tube” (Greek σωλήν), leading to the “receiver” (Arabic ḳābila, Greek ἄγγος or φιάλη) container.\n\n",
        "content": "The complete distilling apparatus consists of three parts: the “cucurbit” (Arabic ḳarʿa, Greek βίκος), the still pot containing the liquid to be distilled, which is heated by a flame; the “head” or “cap” (Arabic anbiḳ, Greek ἄμβιξ) which fits over the mouth of the cucurbit to receive the vapors, with an attached downward-sloping “tube” (Greek σωλήν), leading to the “receiver” (Arabic ḳābila, Greek ἄγγος or φιάλη) container.\n\n\n\nRetorts have the “cap” and the “cucurbit” made into one. The anbik is also called the raʾs (head) of the cucurbit. The liquid in the cucurbit is heated or boiled; the vapour rises into the anbik, where it cools by contact with the walls and condenses, running down the spout into the receiver. A modern descendant of the alembic is the pot still, used to produce distilled beverages.\n\nOriginally from Alembic - Wikipedia\n",
        "url": "/general/2016/08/27/example-post-one/"
      },
    
      {
        "title": "History of the Alembic",
        "excerpt": "Dioscorides’ ambix (described in his De materia medica) is a helmet-shaped lid for gathering condensed mercury. For Athenaeus (~ 225 C.E.) it is a bottle or flask. For later chemists it denotes various parts of crude distillation devices.\n\n",
        "content": "Dioscorides’ ambix (described in his De materia medica) is a helmet-shaped lid for gathering condensed mercury. For Athenaeus (~ 225 C.E.) it is a bottle or flask. For later chemists it denotes various parts of crude distillation devices.\n\n\n\nAlembic drawings appear in works of Cleopatra the Alchemist, Synesius, and Zosimos of Panopolis. There were alembics with two (dibikos) and three (tribikos) receivers.[4] According to Zosimos of Panopolis, the alembic was invented by Mary the Jewess.[5]\n\nThe anbik is described by Ibn al-Awwam in his Kitab al-Filaha (Book of Agriculture), where he explains how rose-water is distilled. Amongst others, it is mentioned in the Mafatih al-Ulum (Key of Sciences) of Khwarizmi and the Kitab al-Asrar (Book of Secrets) of Al-Razi. Some illustrations occur in the Latin translations of works which are attributed to Geber.[2]\n\nOriginally from Alembic - Wikipedia\n",
        "url": "/history/2016/08/28/example-post-two/"
      },
    
      {
        "title": "Description of a Pot Still",
        "excerpt": "A pot still is a type of still used in distilling spirits such as whisky or brandy. Heat is applied directly to the pot containing the wash (for whisky) or wine (for brandy).\n",
        "content": "A pot still is a type of still used in distilling spirits such as whisky or brandy. Heat is applied directly to the pot containing the wash (for whisky) or wine (for brandy). This is called a batch distillation (as opposed to a continuous distillation).\n\nAt standard atmospheric pressure, alcohol boils at 78 °C (172 °F), while water boils at 100 °C (212 °F). During distillation, the vapour contains more alcohol than the liquid. When the vapours are condensed, the resulting liquid contains a higher concentration of alcohol. In the pot still, the alcohol and water vapour combine with esters and flow from the still through the condensing coil. There they condense into the first distillation liquid, the so-called “low wines”. The low wines have a strength of about 25–35% alcohol by volume, and flow into a second still. It is then distilled a second time to produce the colourless spirit, collected at about 70% alcohol by volume. Colour is added through maturation in an oak aging barrel, and develops over time.\n\nThe modern pot still is a descendant of the alembic, an earlier distillation device.\n",
        "url": "/general/2016/08/29/example-post-three/"
      },
    
      {
        "title": "Welcome to Jekyll!",
        "excerpt": "Note\nI’m just keeping the default page made by jekyll for convience\n\nYou’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\nJekyll requires blog post files to be named according to the following format:\n\nYEAR-MONTH-DAY-title.MARKUP\n\nWhere YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and MARKUP is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.\n\n",
        "content": "Note\nI’m just keeping the default page made by jekyll for convience\n\nYou’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.\n\nJekyll requires blog post files to be named according to the following format:\n\nYEAR-MONTH-DAY-title.MARKUP\n\nWhere YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and MARKUP is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.\n\nJekyll also offers powerful support for code snippets:\n\ndef print_hi(name)\n  puts \"Hi, #{name}\"\nend\nprint_hi('Tom')\n#=&gt; prints 'Hi, Tom' to STDOUT.\n\nCheck out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.\n\n",
        "url": "/jekyll/update/2019/11/06/welcome-to-jekyll/"
      },
    
      {
        "title": "Consciousness and Integrated Information",
        "excerpt": "Consciousness and Integrated Information\nIntegrated Information theory is a theory of consciousness put forth by Giulio Tononi. Below is a very boring PDF of my thoughts related to how I think consciousness interacts with semantic information.\n\nDownload\n\n\n    \n        This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n    &lt;/embed&gt;\n\n\n",
        "content": "Consciousness and Integrated Information\nIntegrated Information theory is a theory of consciousness put forth by Giulio Tononi. Below is a very boring PDF of my thoughts related to how I think consciousness interacts with semantic information.\n\nDownload\n\n\n    \n        This browser does not support PDFs. Please download the PDF to view it: Download PDF.\n    &lt;/embed&gt;\n\n\n",
        "url": "/posts/IIT"
      },
    
      {
        "title": "SOE Data to MNI Space",
        "excerpt": "The SOE data was processed with the intent of doing RSA analysis where the scans were aligned to anatomical space. However, I thought it might be a fun project to look at the functional connectivity of the patients that are already processed. The first step to accomplish this is re-processing the SOE data into MNI space for group analysis!\n\nAFNI Notes\nShana Terai wrote some scripts for processing the SOE data. These are very well documented, and well written. They should be looked at prior to this script, as these are based off of her scripts. I’ve written this largely for my own documentation.\n\nRe-Processing\nSince the patients have already been processed once through Shana’s scripts, I can re-process her files with an updated script.\n\nNeed to re-run the proc_py files and have alignment into MNI space. Shana’s help function is great, and patients need to be queued one at a time. Once in the RCC, jobs should be run with the command\nqueue.SOE.ap.sp [ -s SUBJECT# ] [ -d SESSION# ] [ -r SEMAP#]\n\nNote that the -d and -r options should be left blank so that all scans are reprocessed. This command then submits the proper jobs using the SOE.ap.bySE script which does all the real work. This script can be edited for the SOE data to MNI space by editing the afni_proc.py script call. \nHere is the updated call.\n\nafni_proc.py -subj_id SOE_${subj}_${sesh}_SEmap${SEn}                   \\\n        -dsets $dsets                                                   \\\n        -copy_anat SOE_${subj}_T1w_acpc_dc_restore_brain_1mm+orig       \\\n        -blip_forward_dset SOE${subj}_SEmap${SEn}_AP.nii.gz             \\\n        -blip_reverse_dset SOE${subj}_SEmap${SEn}_PA.nii.gz             \\\n        -blocks despike tshift align tlrc volreg  mask scale                 \\\n        -volreg_align_e2a                                               \\\n        -tlrc_base MNI152_T1_2009c+tlrc                                 \\\n        -tlrc_NL_warp                                                   \\\n        -volreg_tlrc_warp                                               \\\n        -anat_has_skull no                                              \\\n        -align_opts_aea -giant_move                                     \\\n        -align_epi_strip_method 3dSkullStrip\n\nThe only change I made was adding the three tlrc commands which were taken from the AFNI proc_py documentation. The only other editing that I made to Shana’s script was chanigng the output_dir variable to a directory under my username.\n\nThe next steps are to re-process the patients and use nilearn for some ICA!\n\nNotes\n\n  \n    Shana has the check for SE maps after the job is submitted, which works, but could be in the main script (the queue script)\n  \n  \n    I dont think the original proc_py used the combine option which should optimally combined the steps\n  \n  \n    Not sure why ‘giant_move’ was used instead of some other option\n  \n  \n    Not sure what the ‘-F’ option is in the torque script\n  \n\n\n",
        "content": "The SOE data was processed with the intent of doing RSA analysis where the scans were aligned to anatomical space. However, I thought it might be a fun project to look at the functional connectivity of the patients that are already processed. The first step to accomplish this is re-processing the SOE data into MNI space for group analysis!\n\nAFNI Notes\nShana Terai wrote some scripts for processing the SOE data. These are very well documented, and well written. They should be looked at prior to this script, as these are based off of her scripts. I’ve written this largely for my own documentation.\n\nRe-Processing\nSince the patients have already been processed once through Shana’s scripts, I can re-process her files with an updated script.\n\nNeed to re-run the proc_py files and have alignment into MNI space. Shana’s help function is great, and patients need to be queued one at a time. Once in the RCC, jobs should be run with the command\nqueue.SOE.ap.sp [ -s SUBJECT# ] [ -d SESSION# ] [ -r SEMAP#]\n\nNote that the -d and -r options should be left blank so that all scans are reprocessed. This command then submits the proper jobs using the SOE.ap.bySE script which does all the real work. This script can be edited for the SOE data to MNI space by editing the afni_proc.py script call. \nHere is the updated call.\n\nafni_proc.py -subj_id SOE_${subj}_${sesh}_SEmap${SEn}                   \\\n        -dsets $dsets                                                   \\\n        -copy_anat SOE_${subj}_T1w_acpc_dc_restore_brain_1mm+orig       \\\n        -blip_forward_dset SOE${subj}_SEmap${SEn}_AP.nii.gz             \\\n        -blip_reverse_dset SOE${subj}_SEmap${SEn}_PA.nii.gz             \\\n        -blocks despike tshift align tlrc volreg  mask scale                 \\\n        -volreg_align_e2a                                               \\\n        -tlrc_base MNI152_T1_2009c+tlrc                                 \\\n        -tlrc_NL_warp                                                   \\\n        -volreg_tlrc_warp                                               \\\n        -anat_has_skull no                                              \\\n        -align_opts_aea -giant_move                                     \\\n        -align_epi_strip_method 3dSkullStrip\n\nThe only change I made was adding the three tlrc commands which were taken from the AFNI proc_py documentation. The only other editing that I made to Shana’s script was chanigng the output_dir variable to a directory under my username.\n\nThe next steps are to re-process the patients and use nilearn for some ICA!\n\nNotes\n\n  \n    Shana has the check for SE maps after the job is submitted, which works, but could be in the main script (the queue script)\n  \n  \n    I dont think the original proc_py used the combine option which should optimally combined the steps\n  \n  \n    Not sure why ‘giant_move’ was used instead of some other option\n  \n  \n    Not sure what the ‘-F’ option is in the torque script\n  \n\n\n",
        "url": "/posts/afni_reproc"
      },
    
      {
        "title": "Connectivity Analysis Using Nilearn",
        "excerpt": "SOE Connectivity Exploration\nFirst, prior to being imported in this script, all the SOE data was pre-processed using AFNI. The EPI scans were all transformed into MNI space, and then I converted these to nifti format and placed them in the directory ‘tmp’. For exploratory purposes, there are only 4 patients included with 72 runs across them.\n\nIt seemed natural that one of the first things to do might be see if there are any consistent correlations across all runs.\n\nTo start this, we load the file-paths for all the scans.\n\n# Need to load all aligned EPI Scans\nimport os\nfunc_filenames = os.listdir('tmp')\nfunc_filenames = ['tmp/'+k for k in func_filenames if k.rsplit('.',1)[1]=='nii']\n\n\nWe now have a list with all the file paths. The next step is to load the MSDL atlas which has MNI coordinates for seeds to find different regions.\n\nfrom nilearn import datasets\natlas = datasets.fetch_atlas_msdl(); # This is an atlas with (x,y,z) coordinates in MNI space for different regions\natlas_filename = atlas['maps']; # Loading atlas image stored in 'maps'\nlabels = atlas['labels']; # Loading atlas data stored in 'labels'\n\n\n/home/smazurchuk/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=DeprecationWarning)\n/home/smazurchuk/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py:2358: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n  output = genfromtxt(fname, **kwargs)\n\n\nFrom here, we can create masks from the atlas. This will be done for 10 runs for time. Note: All the confounds (regressors) have been managed by AFNI, so they are not considered by masker. Also, there are many options that could be used such as smoothing\n\nfrom nilearn.input_data import NiftiMapsMasker\nmasker = NiftiMapsMasker(maps_img=atlas_filename, standardize=True, \n                         verbose=1, memory=\"caching\", memory_level=5)\ntime_series=[]\nfor run in range(len(func_filenames)):\n    time_series.append(masker.fit_transform(func_filenames[run])) \n\nNow that all the masks are made, we can create correlation matrices for each patient. For these correlations, we can look at the upper triangular and see if any group has a signifigantly increased average correlation from the rest of the groups\n\nimport scipy.io as sio\nfrom nilearn.connectome import ConnectivityMeasure\nsio.savemat('RawTimeSeries.mat',{'rawTimeSeries':time_series,'labels':labels})\ncorrelation_measure = ConnectivityMeasure(kind='correlation')\ncorrelation_matrix = correlation_measure.fit_transform(time_series)\nsio.savemat('corrMats.mat',{'corrMats':correlation_matrix})\n\n# Display the correlation matrix\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nilearn import plotting\ncorrelation_matrix=correlation_matrix.mean(0)\nnp.fill_diagonal(correlation_matrix, 0) # Mask out the major diagonal\nplotting.plot_matrix(correlation_matrix, labels=labels, colorbar=True,\n                     vmax=0.8, vmin=-0.8, figure=(10,10));\n\nfig=plt.figure(figsize=(10,6))\nplotting.plot_connectome(correlation_matrix,atlas.region_coords,edge_threshold=\"95%\", figure=fig);\n\n\n\n\n\n\n# We could also look at the correlations\nimport matplotlib.pyplot as plt\ndata = np.triu(correlation_matrix.data)\ndata = data[data!=0]\nplt.figure(figsize=(10,6)); plt.grid(True)\nplt.hist(data,bins=30,edgecolor='k'); plt.xlabel('Correlation');plt.ylabel('Count');\n\n\n\n\nfig=plt.figure(figsize=(10,10))\nplotting.view_connectome(correlation_matrix, atlas.region_coords, edge_threshold='95%') \n\n\n\n\n&lt;Figure size 720x720 with 0 Axes&gt;\n\n\n# We want to at correlations above the 95th Percentile\nimport pandas as pd\ndf = pd.DataFrame(columns=['Name', 'Correlation']); cnt=0;\nfor i in range(len(labels)):\n    for j in range(i,len(labels)):\n        if correlation_matrix[i,j] &gt; .65: # find max\n            df.loc[cnt]=[labels[i]+' __ '+labels[j],\n                       correlation_matrix[i,j]]\n            cnt+=1\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Correlation\n    \n  \n  \n    \n      0\n      L Aud __ R Aud\n      0.708733\n    \n    \n      1\n      L Aud __ Motor\n      0.685552\n    \n    \n      2\n      L Aud __ Basal\n      0.661495\n    \n    \n      3\n      L Aud __ L DLPFC\n      0.662773\n    \n    \n      4\n      L Aud __ D ACC\n      0.656200\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      63\n      L TPJ __ Cereb\n      0.652361\n    \n    \n      64\n      R Pars Op __ R Ins\n      0.653438\n    \n    \n      65\n      L Ins __ Cing\n      0.665602\n    \n    \n      66\n      L Ins __ R Ins\n      0.712899\n    \n    \n      67\n      L Ant IPS __ R Ant IPS\n      0.679085\n    \n  \n\n68 rows × 2 columns\n\n\n",
        "content": "SOE Connectivity Exploration\nFirst, prior to being imported in this script, all the SOE data was pre-processed using AFNI. The EPI scans were all transformed into MNI space, and then I converted these to nifti format and placed them in the directory ‘tmp’. For exploratory purposes, there are only 4 patients included with 72 runs across them.\n\nIt seemed natural that one of the first things to do might be see if there are any consistent correlations across all runs.\n\nTo start this, we load the file-paths for all the scans.\n\n# Need to load all aligned EPI Scans\nimport os\nfunc_filenames = os.listdir('tmp')\nfunc_filenames = ['tmp/'+k for k in func_filenames if k.rsplit('.',1)[1]=='nii']\n\n\nWe now have a list with all the file paths. The next step is to load the MSDL atlas which has MNI coordinates for seeds to find different regions.\n\nfrom nilearn import datasets\natlas = datasets.fetch_atlas_msdl(); # This is an atlas with (x,y,z) coordinates in MNI space for different regions\natlas_filename = atlas['maps']; # Loading atlas image stored in 'maps'\nlabels = atlas['labels']; # Loading atlas data stored in 'labels'\n\n\n/home/smazurchuk/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n  warnings.warn(msg, category=DeprecationWarning)\n/home/smazurchuk/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py:2358: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n  output = genfromtxt(fname, **kwargs)\n\n\nFrom here, we can create masks from the atlas. This will be done for 10 runs for time. Note: All the confounds (regressors) have been managed by AFNI, so they are not considered by masker. Also, there are many options that could be used such as smoothing\n\nfrom nilearn.input_data import NiftiMapsMasker\nmasker = NiftiMapsMasker(maps_img=atlas_filename, standardize=True, \n                         verbose=1, memory=\"caching\", memory_level=5)\ntime_series=[]\nfor run in range(len(func_filenames)):\n    time_series.append(masker.fit_transform(func_filenames[run])) \n\nNow that all the masks are made, we can create correlation matrices for each patient. For these correlations, we can look at the upper triangular and see if any group has a signifigantly increased average correlation from the rest of the groups\n\nimport scipy.io as sio\nfrom nilearn.connectome import ConnectivityMeasure\nsio.savemat('RawTimeSeries.mat',{'rawTimeSeries':time_series,'labels':labels})\ncorrelation_measure = ConnectivityMeasure(kind='correlation')\ncorrelation_matrix = correlation_measure.fit_transform(time_series)\nsio.savemat('corrMats.mat',{'corrMats':correlation_matrix})\n\n# Display the correlation matrix\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nilearn import plotting\ncorrelation_matrix=correlation_matrix.mean(0)\nnp.fill_diagonal(correlation_matrix, 0) # Mask out the major diagonal\nplotting.plot_matrix(correlation_matrix, labels=labels, colorbar=True,\n                     vmax=0.8, vmin=-0.8, figure=(10,10));\n\nfig=plt.figure(figsize=(10,6))\nplotting.plot_connectome(correlation_matrix,atlas.region_coords,edge_threshold=\"95%\", figure=fig);\n\n\n\n\n\n\n# We could also look at the correlations\nimport matplotlib.pyplot as plt\ndata = np.triu(correlation_matrix.data)\ndata = data[data!=0]\nplt.figure(figsize=(10,6)); plt.grid(True)\nplt.hist(data,bins=30,edgecolor='k'); plt.xlabel('Correlation');plt.ylabel('Count');\n\n\n\n\nfig=plt.figure(figsize=(10,10))\nplotting.view_connectome(correlation_matrix, atlas.region_coords, edge_threshold='95%') \n\n\n\n\n&lt;Figure size 720x720 with 0 Axes&gt;\n\n\n# We want to at correlations above the 95th Percentile\nimport pandas as pd\ndf = pd.DataFrame(columns=['Name', 'Correlation']); cnt=0;\nfor i in range(len(labels)):\n    for j in range(i,len(labels)):\n        if correlation_matrix[i,j] &gt; .65: # find max\n            df.loc[cnt]=[labels[i]+' __ '+labels[j],\n                       correlation_matrix[i,j]]\n            cnt+=1\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Correlation\n    \n  \n  \n    \n      0\n      L Aud __ R Aud\n      0.708733\n    \n    \n      1\n      L Aud __ Motor\n      0.685552\n    \n    \n      2\n      L Aud __ Basal\n      0.661495\n    \n    \n      3\n      L Aud __ L DLPFC\n      0.662773\n    \n    \n      4\n      L Aud __ D ACC\n      0.656200\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      63\n      L TPJ __ Cereb\n      0.652361\n    \n    \n      64\n      R Pars Op __ R Ins\n      0.653438\n    \n    \n      65\n      L Ins __ Cing\n      0.665602\n    \n    \n      66\n      L Ins __ R Ins\n      0.712899\n    \n    \n      67\n      L Ant IPS __ R Ant IPS\n      0.679085\n    \n  \n\n68 rows × 2 columns\n\n\n",
        "url": "/2019/11/12/nilearn-connectivity/"
      },
    
      {
        "title": "L1 Norm",
        "excerpt": "A somewhat common question that often gets asked in linear regression is: Why do we minimize the square of the residuals? It might seem more natural to simply minimize the absolute value of the residuals.\n\nI remember that when I asked this question some people commented on the fact that the derivative exists for  but not for . This makes it easier to prove that there is a unique analytic solution, but surely in the age of computers this isn’t an issue. It turns out this explaination is just a part of the story.\n\nThe question of why square the errors has been addressed in several posts online 1 2, so I do not intend for this to be original, just my own stab at summarizing the explaination.\n\n\n\nWe will consider the simplest case of univariate linear regression. In this case: \n\n\n\nTo find the best estimates of w, we can use the L2 norm of the loss function, for which we get a new function of , namely, \nThis has a unique standard solution which can be found in the usual way. For linear functions, this is always convex. In practice, we might be concerned with overfiting, and thus need regularization. We thus update the cost function with a regularization term\n\n\n\nA general family of regularization functions is just the norm of the coefficients!\nThis is where L1 regularization is beneficial. We can get a feel for this by considering a an upper limit for the complexity ().\nThis means that we are trying to minimize the cost within the regular unit circle of the norm. In general, L_2 minumums are equally likely to occur anywhere on the edge of the circle, but more likly to occur on the corner of the box!\n\n\n\n\n\n\n  \n    \n      https://stats.stackexchange.com/questions/118/why-square-the-difference-instead-of-taking-the-absolute-value-in-standard-devia &#8617;\n    \n    \n      https://www.benkuhn.net/squared &#8617;\n    \n  \n\n",
        "content": "A somewhat common question that often gets asked in linear regression is: Why do we minimize the square of the residuals? It might seem more natural to simply minimize the absolute value of the residuals.\n\nI remember that when I asked this question some people commented on the fact that the derivative exists for  but not for . This makes it easier to prove that there is a unique analytic solution, but surely in the age of computers this isn’t an issue. It turns out this explaination is just a part of the story.\n\nThe question of why square the errors has been addressed in several posts online 1 2, so I do not intend for this to be original, just my own stab at summarizing the explaination.\n\n\n\nWe will consider the simplest case of univariate linear regression. In this case: \n\n\n\nTo find the best estimates of w, we can use the L2 norm of the loss function, for which we get a new function of , namely, \nThis has a unique standard solution which can be found in the usual way. For linear functions, this is always convex. In practice, we might be concerned with overfiting, and thus need regularization. We thus update the cost function with a regularization term\n\n\n\nA general family of regularization functions is just the norm of the coefficients!\nThis is where L1 regularization is beneficial. We can get a feel for this by considering a an upper limit for the complexity ().\nThis means that we are trying to minimize the cost within the regular unit circle of the norm. In general, L_2 minumums are equally likely to occur anywhere on the edge of the circle, but more likly to occur on the corner of the box!\n\n\n\n\n\n\n  \n    \n      https://stats.stackexchange.com/questions/118/why-square-the-difference-instead-of-taking-the-absolute-value-in-standard-devia &#8617;\n    \n    \n      https://www.benkuhn.net/squared &#8617;\n    \n  \n\n",
        "url": "/posts/l1"
      },
    
      {
        "title": "Phoneme Analysis",
        "excerpt": "This is some preliminary analysis on the phoneme dataset. There is a jupyter notebook with the code to generate the plots in this post, but I thought it might be better to simple put the results in here. First, some general propterties of the data:\n\n\n  \n    \n      Number of Patients\n      87\n    \n    \n      Number of Runs\n      230\n    \n    \n      Number of Patients with &gt;3 runs\n      18\n    \n  \n\n\nNote: A run is 80 tasks not 160\n\nDistribution of Scores\nBelow is the score distrition for the runs (note, this is for 160 when possible)\n\n  \n\nAs a sanity check, I also thought we should look to see the reaction times of all the patients. Plotted here is a histogram for each patient’s reaction time histogram. It is interesting how varied the reaction times are.\n\n  \n\nNatural Clusters\nI thought we should also see of there is any natural clustering in the dataset. To do this, we can take a the score vector for each run and project this onto principle components. In practice, there are specific algorithms which are non-linear and specifically try to cluster the data (specifically, tSNE)\n\n  \n\nNext, I thought I would look to see if the small clusters are indeed the same people. For this analysis, I took all runs which were from a patient who had at least 4 runs and then color coded them. There are 18 patients who met this criteria, and therefore 18 colors in this plot.\n\n  \n\nWe could also replicate this process using the reaction time data\n\n  \n\n",
        "content": "This is some preliminary analysis on the phoneme dataset. There is a jupyter notebook with the code to generate the plots in this post, but I thought it might be better to simple put the results in here. First, some general propterties of the data:\n\n\n  \n    \n      Number of Patients\n      87\n    \n    \n      Number of Runs\n      230\n    \n    \n      Number of Patients with &gt;3 runs\n      18\n    \n  \n\n\nNote: A run is 80 tasks not 160\n\nDistribution of Scores\nBelow is the score distrition for the runs (note, this is for 160 when possible)\n\n  \n\nAs a sanity check, I also thought we should look to see the reaction times of all the patients. Plotted here is a histogram for each patient’s reaction time histogram. It is interesting how varied the reaction times are.\n\n  \n\nNatural Clusters\nI thought we should also see of there is any natural clustering in the dataset. To do this, we can take a the score vector for each run and project this onto principle components. In practice, there are specific algorithms which are non-linear and specifically try to cluster the data (specifically, tSNE)\n\n  \n\nNext, I thought I would look to see if the small clusters are indeed the same people. For this analysis, I took all runs which were from a patient who had at least 4 runs and then color coded them. There are 18 patients who met this criteria, and therefore 18 colors in this plot.\n\n  \n\nWe could also replicate this process using the reaction time data\n\n  \n\n",
        "url": "/posts/phoneme"
      },
    
      {
        "title": "Finding Word Counts",
        "excerpt": "Finding Word Counts\n\nI thought I might make a little post about to get some word counts. If you are in the MCW neurology department, then go ahead and ssh into my computer to get to a directory with a lot of text files.\n\nssh $username@noether.neuro.mcw.edu\ncd /mnt/data/smazurchuk/intern2crea/bookcorpus/out_txts\n\n\nThere are over 16,000 text files in this directory which are free books. To get word counts, we can simply use Grep!\n\nThe grep command for $word is:\n\ngrep -r -o -i -w $word | wc -l\n\n\nnote: word is allowed to be a string with a space in it\n\nThe grep options are:\n\n\n  \n    \n      Option\n      Function\n    \n  \n  \n    \n      -r\n      Just recursively goes through all files in directory tree\n    \n    \n      -o\n      Print matched part of matching line, with each part on separate line\n    \n    \n      -i\n      Ignore case\n    \n    \n      -w\n      Match whole word (cannot be just part of a word)\n    \n    \n      wc\n      Word count\n    \n    \n      -l\n      Tell word could to count the number of lines\n    \n  \n\n\nThe reason for writing the code this way is so that we can get a short python script which can use grep to get wordcounts for a list of words! Below is the code to accomplish just that\n\nwordList=['red','blue','words']; wordCnt=[]*len(wordList)\nfor idx, word in enumerate(wordList):\n    wordCnt[idx] = int(os.popen('grep -r -o -i -w \"'+ word + '\" | wc -l').read())\n\n\nDouble Counts\nThe astute reader might notice that some words will get double counted. That is, we don’t want “baseball bat” in the count for “baseball”. This can be remedied by noticing that one string is a strict substring of the other! We can simply iterate each word in the list across the other words, and if a is a substring of b, then we subtract the count of a from the count of b. You can verify this (“baseball bat” - “baseball”), and the corresponding code is here:\n\nfor idx, tWord in enumerate(wordList):\n    for idx2, word in enumerate(wordList):\n        if tWord in word and idx!=idx2:\n            wordCnt[idx] = wordCnt[idx]-wordCnt[idx2]\n\nHope that helps, and thanks for reading!\n",
        "content": "Finding Word Counts\n\nI thought I might make a little post about to get some word counts. If you are in the MCW neurology department, then go ahead and ssh into my computer to get to a directory with a lot of text files.\n\nssh $username@noether.neuro.mcw.edu\ncd /mnt/data/smazurchuk/intern2crea/bookcorpus/out_txts\n\n\nThere are over 16,000 text files in this directory which are free books. To get word counts, we can simply use Grep!\n\nThe grep command for $word is:\n\ngrep -r -o -i -w $word | wc -l\n\n\nnote: word is allowed to be a string with a space in it\n\nThe grep options are:\n\n\n  \n    \n      Option\n      Function\n    \n  \n  \n    \n      -r\n      Just recursively goes through all files in directory tree\n    \n    \n      -o\n      Print matched part of matching line, with each part on separate line\n    \n    \n      -i\n      Ignore case\n    \n    \n      -w\n      Match whole word (cannot be just part of a word)\n    \n    \n      wc\n      Word count\n    \n    \n      -l\n      Tell word could to count the number of lines\n    \n  \n\n\nThe reason for writing the code this way is so that we can get a short python script which can use grep to get wordcounts for a list of words! Below is the code to accomplish just that\n\nwordList=['red','blue','words']; wordCnt=[]*len(wordList)\nfor idx, word in enumerate(wordList):\n    wordCnt[idx] = int(os.popen('grep -r -o -i -w \"'+ word + '\" | wc -l').read())\n\n\nDouble Counts\nThe astute reader might notice that some words will get double counted. That is, we don’t want “baseball bat” in the count for “baseball”. This can be remedied by noticing that one string is a strict substring of the other! We can simply iterate each word in the list across the other words, and if a is a substring of b, then we subtract the count of a from the count of b. You can verify this (“baseball bat” - “baseball”), and the corresponding code is here:\n\nfor idx, tWord in enumerate(wordList):\n    for idx2, word in enumerate(wordList):\n        if tWord in word and idx!=idx2:\n            wordCnt[idx] = wordCnt[idx]-wordCnt[idx2]\n\nHope that helps, and thanks for reading!\n",
        "url": "/2019/12/04/words/"
      },
    
      {
        "title": "Creating a GLM for SOE Data",
        "excerpt": "I thought it might be useful to have a little tutorial about how the GLM maps are created for the SOE data. I am basing this off the current method in Tony’s scripts.\n\nProcessing Scripts\nThe general process of creating -maps for each word requires regressingn out words, nuissance regressors and motion.\n\nBuilding Word and Nuissance Regressors\nThe first step is to make 1d regressor files to feed into afni.Tony has a script, regressors.py which which has several functions:\n\n\n  build_large_file - converts psychopy table into python variable with proper format\n  demean_and_write - groups reaction time into 6 groups (2 per session), and converts reaction times into z-scores\n  word_length   - creates word-length as a nuisance regressor\n  word_stim_time   - creates 1d arrays for target word and all other words\n  copy_anat      - uses 3dcopy to copy anatomy files (not sure why?)\n\n\nAt the end of all this work, we have generated the files\n\nBuilding Motion Regressors\n\nThe script 3dDcon_Preparation.py (which requires python 2) generates the motion regressors. This script has two functions:\n\n  afni_to_nifti - this runs the afni command 3dAFNItoNIFTI where all the pre-processed scans in the SOE320 projects folder are converted to nifti files into Tony’s directory\n  \n    prepare_motion_censor_regressor - this takes the motion files from the pre-processed directory. The first three steps use the function 1d_tool.py and this function performs the following tasks:\n\n    \n      \n        \n          De-meaned motion parameters\n          Removes the mean from 1d files\n        \n        \n          Motion parameter derivatives\n          Creates .1d derivative file\n        \n        \n          Create censor file\n          Takes 1d file and creates a censor 1d file\n        \n        \n          Combine censor results\n          Uses 1deval to set motion outlier as 0.1 (1-step(a - 0.1))\n        \n        \n          Combine multiple censor files\n          Uses 1deval to combine censor files using \n        \n      \n    \n  \n\n\n3dDeconvolve Script\n\nTony then has a script for running 3d deconvolve (3dDcon_SOE_Doug.py) which has the following functions:\n\n  extract_words - creates sorted list of words based on file names in patient directory\n  separate_words - creates several subgroups of words (number of groups fed in as argument)\n  \n    Set_the_deconvolve_line - This is used to create the design matrix which is fed into the next step. The regressors included here are:\n\n    \n      \n        \n          Censor Regressor\n          Motion censor\n        \n        \n          Target word regressor\n          This is the time point for the word of interest (6 time points)\n        \n        \n          All other words\n          This is a regressor for AOW\n        \n        \n          Word length regressor\n          This is a regressor for visual information\n        \n        \n          Response Time regressor\n          This is the reaction time z-score. This is thought to correlate with overall word difficulty. Note: This is split into 6 different groups, so this is really 6 different regressors\n        \n        \n          Motion demean\n          Not really sure?\n        \n        \n          Motion derivitive\n          Not sure how generated\n        \n      \n    \n\n    \n      The other key points of this line are that ‘SPMG1’ is used as the HRF, and the output directory is called GLM_wordmaps. At the end of this script, the line which is created from this command is too long to include below, but the output of this function is a file called X.xmat.1d which is fed into the next function\n    \n  \n  Set_the_Remlfit_line - This takes in the design matrix, the input, and a brain mask. The output of this command will look something like:\n\n\n$ 3dREMLfit -matrix .../X.xmat.1D \n   -input \" \" \n   -mask .../SOE_123.mask.nii.gz \n   -fout -rout -tout -Rbuck /jtong/SOE/GLM_wordmaps/Doug_method/123/stats.nii.gz \n   -verb\n\n\nRunning the Script\nFinally, we can now process a patient. We can use Tony’s script build_torque_Doug.py which creates and submits a job to the cluster. The final command is:\n\npython3 build_torque_Doug.py SOE_123\n\nThoughts / Q’s\n\n  Why is SMPG1 used instead of a multiple basis functions (seems more common in the literature)?\n  How/Why to use -censor_prev_TR option in 1d_tool.py (motion censor script)\n\n",
        "content": "I thought it might be useful to have a little tutorial about how the GLM maps are created for the SOE data. I am basing this off the current method in Tony’s scripts.\n\nProcessing Scripts\nThe general process of creating -maps for each word requires regressingn out words, nuissance regressors and motion.\n\nBuilding Word and Nuissance Regressors\nThe first step is to make 1d regressor files to feed into afni.Tony has a script, regressors.py which which has several functions:\n\n\n  build_large_file - converts psychopy table into python variable with proper format\n  demean_and_write - groups reaction time into 6 groups (2 per session), and converts reaction times into z-scores\n  word_length   - creates word-length as a nuisance regressor\n  word_stim_time   - creates 1d arrays for target word and all other words\n  copy_anat      - uses 3dcopy to copy anatomy files (not sure why?)\n\n\nAt the end of all this work, we have generated the files\n\nBuilding Motion Regressors\n\nThe script 3dDcon_Preparation.py (which requires python 2) generates the motion regressors. This script has two functions:\n\n  afni_to_nifti - this runs the afni command 3dAFNItoNIFTI where all the pre-processed scans in the SOE320 projects folder are converted to nifti files into Tony’s directory\n  \n    prepare_motion_censor_regressor - this takes the motion files from the pre-processed directory. The first three steps use the function 1d_tool.py and this function performs the following tasks:\n\n    \n      \n        \n          De-meaned motion parameters\n          Removes the mean from 1d files\n        \n        \n          Motion parameter derivatives\n          Creates .1d derivative file\n        \n        \n          Create censor file\n          Takes 1d file and creates a censor 1d file\n        \n        \n          Combine censor results\n          Uses 1deval to set motion outlier as 0.1 (1-step(a - 0.1))\n        \n        \n          Combine multiple censor files\n          Uses 1deval to combine censor files using \n        \n      \n    \n  \n\n\n3dDeconvolve Script\n\nTony then has a script for running 3d deconvolve (3dDcon_SOE_Doug.py) which has the following functions:\n\n  extract_words - creates sorted list of words based on file names in patient directory\n  separate_words - creates several subgroups of words (number of groups fed in as argument)\n  \n    Set_the_deconvolve_line - This is used to create the design matrix which is fed into the next step. The regressors included here are:\n\n    \n      \n        \n          Censor Regressor\n          Motion censor\n        \n        \n          Target word regressor\n          This is the time point for the word of interest (6 time points)\n        \n        \n          All other words\n          This is a regressor for AOW\n        \n        \n          Word length regressor\n          This is a regressor for visual information\n        \n        \n          Response Time regressor\n          This is the reaction time z-score. This is thought to correlate with overall word difficulty. Note: This is split into 6 different groups, so this is really 6 different regressors\n        \n        \n          Motion demean\n          Not really sure?\n        \n        \n          Motion derivitive\n          Not sure how generated\n        \n      \n    \n\n    \n      The other key points of this line are that ‘SPMG1’ is used as the HRF, and the output directory is called GLM_wordmaps. At the end of this script, the line which is created from this command is too long to include below, but the output of this function is a file called X.xmat.1d which is fed into the next function\n    \n  \n  Set_the_Remlfit_line - This takes in the design matrix, the input, and a brain mask. The output of this command will look something like:\n\n\n$ 3dREMLfit -matrix .../X.xmat.1D \n   -input \" \" \n   -mask .../SOE_123.mask.nii.gz \n   -fout -rout -tout -Rbuck /jtong/SOE/GLM_wordmaps/Doug_method/123/stats.nii.gz \n   -verb\n\n\nRunning the Script\nFinally, we can now process a patient. We can use Tony’s script build_torque_Doug.py which creates and submits a job to the cluster. The final command is:\n\npython3 build_torque_Doug.py SOE_123\n\nThoughts / Q’s\n\n  Why is SMPG1 used instead of a multiple basis functions (seems more common in the literature)?\n  How/Why to use -censor_prev_TR option in 1d_tool.py (motion censor script)\n\n",
        "url": "/2019/12/05/GLM/"
      },
    
      {
        "title": "Linear Regression and Linear Classification",
        "excerpt": "This post is to detail the similarities between linear regression and linear classification techniques. First, we will consider some data for linear regression which falls into two categories.\n\nConsider some data:\n\n\n  \n    \n      X1\n      X2\n      Category\n    \n  \n  \n    \n      0.11\n      0.96\n      1\n    \n    \n      0.2\n      0.57\n      1\n    \n    \n      3.29\n      4.22\n      2\n    \n    \n      5.02\n      6\n      2\n    \n    \n      6.08\n      7.13\n      2\n    \n    \n      0.60\n      1.24\n      1\n    \n    \n      3.96\n      4.08\n      2\n    \n    \n      3.16\n      3.48\n      2\n    \n    \n      -0.50\n      0.73\n      1\n    \n    \n      2.28\n      2.490\n      1\n    \n    \n      0.56\n      1.73\n      1\n    \n    \n      5.24\n      6\n      2\n    \n    \n      1.50\n      2.900\n      1\n    \n    \n      3.68\n      3.79\n      2\n    \n    \n      0.86\n      1.48\n      1\n    \n  \n\n\n\n\nTo copy these variables, paste the following lines into matlab\nX = [0.110705312404362,0.962036338238243;0.195536168994095,0.567433726105442;3.29080172573339,4.21580925766402;5.02219607197230,5.99689256969713;6.07542982055026,7.13347594983259;0.599994589792336,1.23715662880782;3.96292001423081,4.08146134381088;3.15718188586238,3.48100422049278;-0.561666701284046,0.729987382373674;2.27606954894299,2.49156460997681;0.562861314317448,1.73074288392112;5.23577426251946,5.99710543566519;1.49624905326917,2.90499627947359;3.68201519547809,3.79257208848654;0.858292592800166,1.48433420568446];\nr = [0.6350 0.0780 0.1840]; b=[0 0.4470 0.7410]; c = [b; b; r; r; r; b; r; r; b; b; b; r; b; r; b];\n\n\nAbove, the regression can be thought of as finding some w’s such that:\n\n\n\nFrom here, one can use a closed form solution which follows:\n\n\n\nIn matlab, this equation can be implemented as follows:\n\nd = [X(:,1),ones(15,1)];  % add a ones column for offset\nw = inv(d'*d)*d'*X(:,2);\nhold on\nscatter(X(:,1),X(:,2),[],c,'filled')\nplot(X(:,1),w'*d')\nhold off\n\n\n\nClassification\nIn order to change the above regression problem into a classification problem, we can simply use the columns X1 and X2 to regress out a third column (Category). After this, we can simply set a threshold (naivly, maybe 1.6) to classify!\nWe might also want to look at the decision boundary, and this can be done by using the solved weights. The code to do this in matlab is:\n\ng = [1,1,2,2,2,1,2,2,1,1,1,2,1,2,1];\nd2 = [X, ones(15,1)];\nw2 = inv(d2'*d2)*d2'*g';\ng_hat = w2'*d2';\ncolors = zeros(15,3);\nfor i=1:15 \n    if g_hat(i)&lt;1.6 \n        colors(i,:)=b; \n    else\n        colors(i,:)=r;\n    end \nend \nhold on\nscatter(X(:,1),X(:,2),[],colors,'filled')\nfimplicit(@(x,y) w2(1)*x + w2(2)*y +w2(3)-1.6)\nhold off\n\n\n\nConclusion\nIn short, as you expect, for linearly seperable data, we can model linear regression and linear classification as similar processes\n\n",
        "content": "This post is to detail the similarities between linear regression and linear classification techniques. First, we will consider some data for linear regression which falls into two categories.\n\nConsider some data:\n\n\n  \n    \n      X1\n      X2\n      Category\n    \n  \n  \n    \n      0.11\n      0.96\n      1\n    \n    \n      0.2\n      0.57\n      1\n    \n    \n      3.29\n      4.22\n      2\n    \n    \n      5.02\n      6\n      2\n    \n    \n      6.08\n      7.13\n      2\n    \n    \n      0.60\n      1.24\n      1\n    \n    \n      3.96\n      4.08\n      2\n    \n    \n      3.16\n      3.48\n      2\n    \n    \n      -0.50\n      0.73\n      1\n    \n    \n      2.28\n      2.490\n      1\n    \n    \n      0.56\n      1.73\n      1\n    \n    \n      5.24\n      6\n      2\n    \n    \n      1.50\n      2.900\n      1\n    \n    \n      3.68\n      3.79\n      2\n    \n    \n      0.86\n      1.48\n      1\n    \n  \n\n\n\n\nTo copy these variables, paste the following lines into matlab\nX = [0.110705312404362,0.962036338238243;0.195536168994095,0.567433726105442;3.29080172573339,4.21580925766402;5.02219607197230,5.99689256969713;6.07542982055026,7.13347594983259;0.599994589792336,1.23715662880782;3.96292001423081,4.08146134381088;3.15718188586238,3.48100422049278;-0.561666701284046,0.729987382373674;2.27606954894299,2.49156460997681;0.562861314317448,1.73074288392112;5.23577426251946,5.99710543566519;1.49624905326917,2.90499627947359;3.68201519547809,3.79257208848654;0.858292592800166,1.48433420568446];\nr = [0.6350 0.0780 0.1840]; b=[0 0.4470 0.7410]; c = [b; b; r; r; r; b; r; r; b; b; b; r; b; r; b];\n\n\nAbove, the regression can be thought of as finding some w’s such that:\n\n\n\nFrom here, one can use a closed form solution which follows:\n\n\n\nIn matlab, this equation can be implemented as follows:\n\nd = [X(:,1),ones(15,1)];  % add a ones column for offset\nw = inv(d'*d)*d'*X(:,2);\nhold on\nscatter(X(:,1),X(:,2),[],c,'filled')\nplot(X(:,1),w'*d')\nhold off\n\n\n\nClassification\nIn order to change the above regression problem into a classification problem, we can simply use the columns X1 and X2 to regress out a third column (Category). After this, we can simply set a threshold (naivly, maybe 1.6) to classify!\nWe might also want to look at the decision boundary, and this can be done by using the solved weights. The code to do this in matlab is:\n\ng = [1,1,2,2,2,1,2,2,1,1,1,2,1,2,1];\nd2 = [X, ones(15,1)];\nw2 = inv(d2'*d2)*d2'*g';\ng_hat = w2'*d2';\ncolors = zeros(15,3);\nfor i=1:15 \n    if g_hat(i)&lt;1.6 \n        colors(i,:)=b; \n    else\n        colors(i,:)=r;\n    end \nend \nhold on\nscatter(X(:,1),X(:,2),[],colors,'filled')\nfimplicit(@(x,y) w2(1)*x + w2(2)*y +w2(3)-1.6)\nhold off\n\n\n\nConclusion\nIn short, as you expect, for linearly seperable data, we can model linear regression and linear classification as similar processes\n\n",
        "url": "/2020/01/08/linear-class/"
      },
    
      {
        "title": "The 'Topy' of Language",
        "excerpt": "I thought I might make a post of an idea that has been circulating in my head this past week. The idea centers on extending what somatopy, retinotopy, and tonotopy imply for concept representation.\n\nBackground\nSomatotopy is known to exist throughout many regions of the brain. Classically, we know about the humunculus in the motor gyrus, but it is a much more general theme that neurons close together in the cortex tend to represent similar things. For a more formal definition of somatotopy, we can define it as:\n\n\n  Somatotopy is the point-for-point correspondence of an area of the body to a specific point on the central nervous system 1\n\n\nPractically speaking, this generalizes to the retina and cochlea resulting in respective retinotopic and tonotopic maps. These maps can be explored through functional imaging as shown in the figures below:\n\n\nPolar angle represented across the visual cortex\n\n\n Demonstrates a down-sweep of tones \n\nBoth of these examples are taken from the website of Marty Sereno. A general question is:\n\n\n  Are there parameters which vary continuously vary in the language system?\n\n\nIf there are, how might we go about finding and exploring them?\n\nGeneral Approach\nWhat is described above can also be thought of as a degree for the system. In the visual cortex, the two largest of degrees of freedom would (hopefully) be eccentricity and angle. The general problem of finding degrees of freedom for a system corresponds to dimensionality reduction. In most cases, we are interested in non-linear dimensionality reduction, for which many algorithms exist. One of the most common techniques is known as diffusion mapping, so we might try to recover eccentricity and angle in the visual cortex using this method.\n\nDataset\n\nLanguage System\nFor a general idea of which parts of the brain are involved in concept representation, we can look at computer-generated meta-analysis found at NeuroSynth.\n\n\n\nWe can use this mask to select voxels of interest in MNI space. This list of masked -values can then be thought of as a vector for which we can look as the question:\n\n\n  Are there any continuous degrees of freedom exhibited by the collection of vectors?\n\n\nTo do this in python, first, all the t-scores were converted into MNI space, and then the neurosynth mask was scaled and applied to the voxels.\n\nimport os\nimport numpy as np\nimport subprocess as sp\nimport nibabel as nib\n\nlangMask = nib.load('language_association-test_z_FDR_0.01.nii.gz') #Neurosynth-Mask\n\nvolnames = list(set([k.rsplit('.',1)[0] for k in os.listdir('alignedData')]))\nwordLabels = []; patLabel = []; wordVecs = []\nfor vol in volnames:\n    g    = nib.load('alignedData/'+vol+'.HEAD')\n    gg   = nib.brikhead.AFNIHeader(nib.brikhead.parse_AFNI_header('alignedData/'+vol+'.HEAD'))\n    wordLabels.extend(gg.get_volume_labels()); patLabel.extend([vol.rsplit('.')[0]]*320)\n    resamp_mask = nil.image.resample_to_img(langMask,g)\n    wordVecs.append( g.get_fdata()[resamp_mask.dataobj &gt;2,:].T ) #Threshold neurosynth mask at Z&gt;2\nallVecs = np.concatenate(wordVecs,axis=0)\n\nThe result of this is a 34,000 dimensional vector for each word in each patient which gives us a data matrix of [7,000 by 34,000]. This can then be analyzed using dimensionality reduction techniques such as PCA, tSNE, and Diffusion mapping. Unfortunately, after running these analysis, there was no ‘naturual’ ordering to the words, so for the moment, this is where my analysis ends.\n\nReferences\n\n  \n    \n      Saladin, Kenneth (2012). Anatomy and Physiology. New York: McGraw Hill. pp. 541, 542. &#8617;\n    \n  \n\n",
        "content": "I thought I might make a post of an idea that has been circulating in my head this past week. The idea centers on extending what somatopy, retinotopy, and tonotopy imply for concept representation.\n\nBackground\nSomatotopy is known to exist throughout many regions of the brain. Classically, we know about the humunculus in the motor gyrus, but it is a much more general theme that neurons close together in the cortex tend to represent similar things. For a more formal definition of somatotopy, we can define it as:\n\n\n  Somatotopy is the point-for-point correspondence of an area of the body to a specific point on the central nervous system 1\n\n\nPractically speaking, this generalizes to the retina and cochlea resulting in respective retinotopic and tonotopic maps. These maps can be explored through functional imaging as shown in the figures below:\n\n\nPolar angle represented across the visual cortex\n\n\n Demonstrates a down-sweep of tones \n\nBoth of these examples are taken from the website of Marty Sereno. A general question is:\n\n\n  Are there parameters which vary continuously vary in the language system?\n\n\nIf there are, how might we go about finding and exploring them?\n\nGeneral Approach\nWhat is described above can also be thought of as a degree for the system. In the visual cortex, the two largest of degrees of freedom would (hopefully) be eccentricity and angle. The general problem of finding degrees of freedom for a system corresponds to dimensionality reduction. In most cases, we are interested in non-linear dimensionality reduction, for which many algorithms exist. One of the most common techniques is known as diffusion mapping, so we might try to recover eccentricity and angle in the visual cortex using this method.\n\nDataset\n\nLanguage System\nFor a general idea of which parts of the brain are involved in concept representation, we can look at computer-generated meta-analysis found at NeuroSynth.\n\n\n\nWe can use this mask to select voxels of interest in MNI space. This list of masked -values can then be thought of as a vector for which we can look as the question:\n\n\n  Are there any continuous degrees of freedom exhibited by the collection of vectors?\n\n\nTo do this in python, first, all the t-scores were converted into MNI space, and then the neurosynth mask was scaled and applied to the voxels.\n\nimport os\nimport numpy as np\nimport subprocess as sp\nimport nibabel as nib\n\nlangMask = nib.load('language_association-test_z_FDR_0.01.nii.gz') #Neurosynth-Mask\n\nvolnames = list(set([k.rsplit('.',1)[0] for k in os.listdir('alignedData')]))\nwordLabels = []; patLabel = []; wordVecs = []\nfor vol in volnames:\n    g    = nib.load('alignedData/'+vol+'.HEAD')\n    gg   = nib.brikhead.AFNIHeader(nib.brikhead.parse_AFNI_header('alignedData/'+vol+'.HEAD'))\n    wordLabels.extend(gg.get_volume_labels()); patLabel.extend([vol.rsplit('.')[0]]*320)\n    resamp_mask = nil.image.resample_to_img(langMask,g)\n    wordVecs.append( g.get_fdata()[resamp_mask.dataobj &gt;2,:].T ) #Threshold neurosynth mask at Z&gt;2\nallVecs = np.concatenate(wordVecs,axis=0)\n\nThe result of this is a 34,000 dimensional vector for each word in each patient which gives us a data matrix of [7,000 by 34,000]. This can then be analyzed using dimensionality reduction techniques such as PCA, tSNE, and Diffusion mapping. Unfortunately, after running these analysis, there was no ‘naturual’ ordering to the words, so for the moment, this is where my analysis ends.\n\nReferences\n\n  \n    \n      Saladin, Kenneth (2012). Anatomy and Physiology. New York: McGraw Hill. pp. 541, 542. &#8617;\n    \n  \n\n",
        "url": "/2020/01/14/somatotopy-language/"
      },
    
      {
        "title": "Distance Metrics and Noise",
        "excerpt": "Representational Similarity Analysis (RSA) is a widely used data analysis method used in fMRI. It is a form of Multi-Voxel Pattern Analysis (MVPA). In essence, it utilizes dissimilarity matrices to whether two datasets are expressing the same underlying information. Critical to this method is the underlying choice of distance metrics.\n\nThere are many ways to calculate ‘Distances’ between two vectors. Some of the most familiar are euclidean, correlation, and others. In Kriegesckorte’s original paper, he reccomends using the Mahalanobis distance.\n\nDistance Metrics\n\nStarting broadly, in mathematics a metric is a function that defines a distance (~mapping to the reals) between each pair of elements of a set which meets 4 conditions. The details of this are outside the scope of this post, but it is worth pointing out that the metrics of most common study are of a bilinear form and are called Metric Tensors. This means that the metric follows a general form of:\n\n\n\nCross-Validated Mahalanobis\nThe central idea that underlies cross-validated mahalanobis is that you have 2 estimates of the difference between the vectors. I wrote a matlab script to calculate the cross-validated mahalanobis distance between points in the presence of noise.\n\nX = 5*rand(100,20);      % Generate 100, 20 dimensional points (true signal)\ns1 = X+randn(100,20);    % Add normally distributed noise to signal\ns2 = X+randn(100,20);    % Add normally distributed noise to signal\n\ns1 = s1*(cov(s1)^(-1/2)); % Normalize signal by covariance matrix\ns2 = s2*(cov(s2)^(-1/2)); % Normalize signal by covariance matrix\n\nfor j=1:100\n    for k=1:100\n        d(j,k) = (s1(j,:)-s1(k,:))*(s2(j,:)-s2(k,:))'; % Calculate pairwise distance matrix\n    end\nend\n\nestimated_distance = d(tril(ones(size(d)) == 1, -1));  % Get just the lower triangular part\ntrue_distance      = pdist(X, 'mahalanobis');\n\n\nExamples\n\nHere, I used Matlab to get a better feel for how these metrics respond to noise. The first major point is that most distance metrics overestimate distances in the presence of noise. An exception to this rule is the cross-validated Mahalanobis distance.\n\n \n We can see that euclidean distance OVER estimates with noise, but other distance metrics are less sensitive to noise \n\nWhile there is a clear difference between some of the distance metrics, we might wonder if this makes a difference? To do this, we can perform a mantel test on dissimilarity matrices produced from pure signal, and in the presence of noise.\n\n  \n Mantel test in the presence of noise. It is important to note that there are TWICE as many measurements that go into the cross-validated distance metrics as compared to the other distances \n\nWe can see noise seperates out the metrics. We see that the euclidean distance over-estimates distances in the presence of noise, it strongly outperforms pearson correlation. Standardized euclidean happens to give identical results as cosine for our particular toy data. We also see that for our purposes, general mahalanobis performs the same as euclidean or cosine.\n\nA matlab script which will generate these two figures can be found here:\n\nDistance Analysis\n\n",
        "content": "Representational Similarity Analysis (RSA) is a widely used data analysis method used in fMRI. It is a form of Multi-Voxel Pattern Analysis (MVPA). In essence, it utilizes dissimilarity matrices to whether two datasets are expressing the same underlying information. Critical to this method is the underlying choice of distance metrics.\n\nThere are many ways to calculate ‘Distances’ between two vectors. Some of the most familiar are euclidean, correlation, and others. In Kriegesckorte’s original paper, he reccomends using the Mahalanobis distance.\n\nDistance Metrics\n\nStarting broadly, in mathematics a metric is a function that defines a distance (~mapping to the reals) between each pair of elements of a set which meets 4 conditions. The details of this are outside the scope of this post, but it is worth pointing out that the metrics of most common study are of a bilinear form and are called Metric Tensors. This means that the metric follows a general form of:\n\n\n\nCross-Validated Mahalanobis\nThe central idea that underlies cross-validated mahalanobis is that you have 2 estimates of the difference between the vectors. I wrote a matlab script to calculate the cross-validated mahalanobis distance between points in the presence of noise.\n\nX = 5*rand(100,20);      % Generate 100, 20 dimensional points (true signal)\ns1 = X+randn(100,20);    % Add normally distributed noise to signal\ns2 = X+randn(100,20);    % Add normally distributed noise to signal\n\ns1 = s1*(cov(s1)^(-1/2)); % Normalize signal by covariance matrix\ns2 = s2*(cov(s2)^(-1/2)); % Normalize signal by covariance matrix\n\nfor j=1:100\n    for k=1:100\n        d(j,k) = (s1(j,:)-s1(k,:))*(s2(j,:)-s2(k,:))'; % Calculate pairwise distance matrix\n    end\nend\n\nestimated_distance = d(tril(ones(size(d)) == 1, -1));  % Get just the lower triangular part\ntrue_distance      = pdist(X, 'mahalanobis');\n\n\nExamples\n\nHere, I used Matlab to get a better feel for how these metrics respond to noise. The first major point is that most distance metrics overestimate distances in the presence of noise. An exception to this rule is the cross-validated Mahalanobis distance.\n\n \n We can see that euclidean distance OVER estimates with noise, but other distance metrics are less sensitive to noise \n\nWhile there is a clear difference between some of the distance metrics, we might wonder if this makes a difference? To do this, we can perform a mantel test on dissimilarity matrices produced from pure signal, and in the presence of noise.\n\n  \n Mantel test in the presence of noise. It is important to note that there are TWICE as many measurements that go into the cross-validated distance metrics as compared to the other distances \n\nWe can see noise seperates out the metrics. We see that the euclidean distance over-estimates distances in the presence of noise, it strongly outperforms pearson correlation. Standardized euclidean happens to give identical results as cosine for our particular toy data. We also see that for our purposes, general mahalanobis performs the same as euclidean or cosine.\n\nA matlab script which will generate these two figures can be found here:\n\nDistance Analysis\n\n",
        "url": "/2020/01/16/noisy-distances/"
      },
    
      {
        "title": "Mutual Information",
        "excerpt": "Mutual information is a metric used extensively across many fields.\n\nIntuition\nFirst, prior to talking definitions and nuance, we can get a general sense for what we are trying to measure. Given two sets of observations, we might wonder whether the observations are independent. One indication that they may not be independent is if they are correlated. However, correlation only captures a linear dependence. A more general sense of what is sought is whether knowing the value of observation, provides any information about the value of the second random variable.\n\nThis intuition tells us that for two independent variables, the joint distribution of two variables should simply be the product of the two marginal distributions. Assume for two random variables A and B and observations  and \n\n\n\nEquivalently\n\n\n\nThis means that two variables are dependent it the above is not the case! In general, our goal is to come up with a method of generating a number (preferably between 0 and 1) which tells us how far away from completely independent two distributions are. Since entropy is designed to be additive between two independent distributions, a metric might feel something along the lines of the following::\n\n\n\nWhich might read something along the lines of “The information gain equals the entropy of the joint distribution over the sum of entropies of the marginal distributions”. In this case a value of 1 indicates independence, and values closer two zero indicate more dependence.\n\nFormal Definition\n\nWikipedia gives the following definition:\n\n\n  The Mutual Information of two random variables is a measure of the mutual dependence between the two variables.\n\n\nFor two discrete random variables, it can be calculated as:\n\n\n\nwhere  is the joint distribution of  and , and  and  are the marginal probability mass functions of  and  respectively 1\n\nThis definition seems easy enough to apply. However, one thing which one quickly realizes when using this metric is that calculating the probability mass(or density) functions requires picking a bin size (or kernel smoothing parameter)! One might notice this and hope that there is a quick and natural choice, or that bin size doesn’t change the results too much, but unfortunately, neither of those things are the case!\n\nMatlab Implementation\nFollowing the above definition, I wrote some code to calculate the mutual information for two random variables (A and B):\n\nn=6; % # of bins for making PMF\ng1 = histcounts(A,n); % Bin the data\ng2 = histcounts(B,n);\ng3 = histcounts2(A,B,n);\npmf1 = repmat(g1/sum(g1),n,1); % change counts to %, and make grid for sum\npmf2 = repmat((g2/sum(g2))',1,n);\npmf3 = g3/sum(g3(:));\npmf1(pmf3==0)=[]; pmf2(pmf3==0)=[]; pmf3(pmf3==0)=[]; % remove 0 entries\nMI = sum(pmf3.*log(pmf3./(pmf1.*pmf2))) % Calculate MI score\n\nNote, I naively choose to uniform number and spacing of bins. From here we can generate some test data and see the results!\n\nCircle\n\nBin Spacing Modification\nIt is important to note that to give our algorithm the best chance of success, I use non-uniform bin size with bins chosen from the joint distribution. We can look at the dependence on the bin size\n\nKNN Approach\n\nApplications\n\nA 2018 paper which I personally think is fascinating makes the claim that:\n\n\n  Framing word embedding as metric recovery of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. 2\n\n\nThe connection between metric recovery and mutual information is that a seminal paper demonstrated that human word ratings are linearly related to many distributionaly derived pointwise mutual information (PMI) 3. Following this motivation, the paper mentions how in recent times, it has been shown that many current distributional models are related to eachother through PMI. To quote the paper:\n\n\n  In terms of algorithms, Levy and Goldberg (2014b) demonstrated that the global minimum of the skip-gram method with negative sampling of Mikolov et al. (2013b) implicitly factorizes a shifted version of the PMI matrix of word-context pairs.\n\n\nI mention these results here because I think that with the prevalence of interesting results using mutual information in so many fields, we should be aware of the nuance in actually calculating its value!\n\nReferences\n\n  \n    \n      https://en.wikipedia.org/wiki/Mutual_information#In_terms_of_PMFs_for_discrete_distributions &#8617;\n    \n    \n      Hashimoto, T. B., Alvarez-Melis, D., &amp; Jaakkola, T. S. (2018). Word Embeddings as Metric Recovery in Semantic Spaces. Transactions of the Association for Computational Linguistics, 4, 273–286. doi: 10.1162/tacl_a_00098 &#8617;\n    \n    \n      Kenneth Ward Church andPatrick Hanks. 1990. Word association norms, mutual information, and lexicography.Comput. Linguist.,16(1):22–29 &#8617;\n    \n  \n\n",
        "content": "Mutual information is a metric used extensively across many fields.\n\nIntuition\nFirst, prior to talking definitions and nuance, we can get a general sense for what we are trying to measure. Given two sets of observations, we might wonder whether the observations are independent. One indication that they may not be independent is if they are correlated. However, correlation only captures a linear dependence. A more general sense of what is sought is whether knowing the value of observation, provides any information about the value of the second random variable.\n\nThis intuition tells us that for two independent variables, the joint distribution of two variables should simply be the product of the two marginal distributions. Assume for two random variables A and B and observations  and \n\n\n\nEquivalently\n\n\n\nThis means that two variables are dependent it the above is not the case! In general, our goal is to come up with a method of generating a number (preferably between 0 and 1) which tells us how far away from completely independent two distributions are. Since entropy is designed to be additive between two independent distributions, a metric might feel something along the lines of the following::\n\n\n\nWhich might read something along the lines of “The information gain equals the entropy of the joint distribution over the sum of entropies of the marginal distributions”. In this case a value of 1 indicates independence, and values closer two zero indicate more dependence.\n\nFormal Definition\n\nWikipedia gives the following definition:\n\n\n  The Mutual Information of two random variables is a measure of the mutual dependence between the two variables.\n\n\nFor two discrete random variables, it can be calculated as:\n\n\n\nwhere  is the joint distribution of  and , and  and  are the marginal probability mass functions of  and  respectively 1\n\nThis definition seems easy enough to apply. However, one thing which one quickly realizes when using this metric is that calculating the probability mass(or density) functions requires picking a bin size (or kernel smoothing parameter)! One might notice this and hope that there is a quick and natural choice, or that bin size doesn’t change the results too much, but unfortunately, neither of those things are the case!\n\nMatlab Implementation\nFollowing the above definition, I wrote some code to calculate the mutual information for two random variables (A and B):\n\nn=6; % # of bins for making PMF\ng1 = histcounts(A,n); % Bin the data\ng2 = histcounts(B,n);\ng3 = histcounts2(A,B,n);\npmf1 = repmat(g1/sum(g1),n,1); % change counts to %, and make grid for sum\npmf2 = repmat((g2/sum(g2))',1,n);\npmf3 = g3/sum(g3(:));\npmf1(pmf3==0)=[]; pmf2(pmf3==0)=[]; pmf3(pmf3==0)=[]; % remove 0 entries\nMI = sum(pmf3.*log(pmf3./(pmf1.*pmf2))) % Calculate MI score\n\nNote, I naively choose to uniform number and spacing of bins. From here we can generate some test data and see the results!\n\nCircle\n\nBin Spacing Modification\nIt is important to note that to give our algorithm the best chance of success, I use non-uniform bin size with bins chosen from the joint distribution. We can look at the dependence on the bin size\n\nKNN Approach\n\nApplications\n\nA 2018 paper which I personally think is fascinating makes the claim that:\n\n\n  Framing word embedding as metric recovery of a semantic space unifies existing word embedding algorithms, ties them to manifold learning, and demonstrates that existing algorithms are consistent metric recovery methods given co-occurrence counts from random walks. 2\n\n\nThe connection between metric recovery and mutual information is that a seminal paper demonstrated that human word ratings are linearly related to many distributionaly derived pointwise mutual information (PMI) 3. Following this motivation, the paper mentions how in recent times, it has been shown that many current distributional models are related to eachother through PMI. To quote the paper:\n\n\n  In terms of algorithms, Levy and Goldberg (2014b) demonstrated that the global minimum of the skip-gram method with negative sampling of Mikolov et al. (2013b) implicitly factorizes a shifted version of the PMI matrix of word-context pairs.\n\n\nI mention these results here because I think that with the prevalence of interesting results using mutual information in so many fields, we should be aware of the nuance in actually calculating its value!\n\nReferences\n\n  \n    \n      https://en.wikipedia.org/wiki/Mutual_information#In_terms_of_PMFs_for_discrete_distributions &#8617;\n    \n    \n      Hashimoto, T. B., Alvarez-Melis, D., &amp; Jaakkola, T. S. (2018). Word Embeddings as Metric Recovery in Semantic Spaces. Transactions of the Association for Computational Linguistics, 4, 273–286. doi: 10.1162/tacl_a_00098 &#8617;\n    \n    \n      Kenneth Ward Church andPatrick Hanks. 1990. Word association norms, mutual information, and lexicography.Comput. Linguist.,16(1):22–29 &#8617;\n    \n  \n\n",
        "url": "/2020/02/20/pmi/"
      },
    
      {
        "title": "Word Meanings Through Analogies",
        "excerpt": "When I first learned about the ability of distributionally derived word representations to complete analogies I was very surprised. I thought, “surely we are on the cusp of deeply understanding language”. I think that sometimes we get so used to common knowledge that it looses some of its wonder. Why should distributional models perform well at analogies? Shouldn’t analogies be a generally difficult task which require deep understanding? Surely completing analogies requires at least as much semantic information (if not more) as assigning pronouns to their proper referent?\n\nOf course, the field of linguistics has recognized the somewhat counter intuitive result that in-fact the reverse is true. It is surprisingly hard to assign roles to pronouns, but word-ratings have been shown to be capable of completing analogies since the 1970’s! (source)\n\nWinograd Schemas\n\nThe Winograd Schema Challenge (WSC) is a test of machine intelligence that was proposed by Hector Levesque (UoT). Roughly, it is a pair of sentences which contain two noun phrases and an ambiguous pronoun which refers to different noun-phrases in the sentences. The prototypical example is given below:\n\n\n  The city councilmen refused the demonstrators a permit because they feared violence.\n\n\nIn asking who they refers to, it seems a rather simplistic task to assign it to the committee. However, by just changing the last words, we can change the role asignment\n\n\n  The city councilmen refused the demonstrators a permit because they advocated violence.\n\n\nThis demonstrates that what might seem a simple task which can simply be answered through syntactic regularities of language, actually requires understanding the semantics of the sentence.\n\nMotivation\n\nSo, why did I reference analogies and Winograd schemas? Well, it is motivated by the success of word rating models at solving analogies. My hunch is that\n\nLooking for Analogies\nFollowing this line of thought, I considered whether we could derive word meanings through the use of assimilies in literature. For example, the sentence\n\n\n  He was fast like a cheetah\n\n\nTells us a prototypical feature of cheetahs; namely, that they are fast! In general, I considered whether looking for phrases such as “like a” followed by a noun to be able to give insight to the prototypical features we have for different objects. To this end, I searched 16,000 free eBooks for “like a”\n\nstring = \"like a\"\nsentences = os.popen('grep -r -i -w -h \"'+ string + '\"').read().splitlines()\n\n\nThis command found 490,000 examples. So, just randomly clicking through them, what are the examples?\n\n\n  Make me a offer, Mister Beeler. I’d like a offer if you’d care to make one\n\n\nNot quite what I had hoped. The first example uses the work in a literal sense!\n\n\n  and Jean Claude would retreat like a disparaged puppy\n\n\nA puppy retreating? I think puppies are cute and innocent, hardly disparaging or retreating\n\n\n  spires that reached to the heaven like a cathedral of praise, bringing symmetry to its background\n\n\nCathedrals, reaching. Okay, maybe, but still, pretty hard.\n\n\n  tool that the creature Fur-nose carried, like a rock used for opening nuts\n\n\nOne thing that seems to be a trend is that it is not always clear what adjective is referencing the noun following “like a”. If we are to have any hope, we will require a syntatic depedency parser.\n\n\n  Together they watched Paul flitting ahead like a human dragonfly\n\n\nHere, what the noun should be is not very clear! We clearly know that it should be dragonfly, but that is because we recognize human as an adjective and not noun.\n\nWith everything above given, I picked some of the more random examples. There are many examples for which the assimily was clear. For fun, some are included here:\n\n\n  The apartment was furnished, but it didn’t look like a home\n\n\nThis tells that “look” is strongly associated with home! This is great!\n\n\n  We continued going until we arrived at an open place like a plateau with many chambers\n\n\n“As a”\n\nFor the other kind of simily, we find a similar number of raw occurences of “as a”. In looking at the examples, some are:\n\n\n  “I’m not sure,” she said. “I usually read them as a set, not as individual”\n\n\nHere we have a tricky assignment. Are sets (a rather abstract concept) usually visual (as would be indicated by read)?\n\n\n  Tenderness in a man is not viewed as a manly thing\n\n\nOooo, here he have a negation. Another tricky case I had not considered! Not to mention, how should “thing” be handled?\n\n\n  “You served as a vassal of the Queen in my childhood,” Headred said\n\n\nHere I have included a positive example. Vassals serve!\n\n\n  and blow up a transformer without so much as a scratch?” Dodger asked\n\n\nHow would we handle common phrases such as “without so much …”. A tricky problem indeed. On top of the examples given above, one final tantelizing example was:\n\n\n  as ever-present as a heartbeat\n\n\nI think that this does tell us something deep about how we represent the concept “heartbeat”. But how is it captured?\n\nThere is an interesting idea that we might learn about abstract concepts through the use of metaphors. Metaphores allow us to extend our embdied knowledge in useful ways. Maybe we can come up with a Reg-Ex expression to search for metaphores. This is i\n",
        "content": "When I first learned about the ability of distributionally derived word representations to complete analogies I was very surprised. I thought, “surely we are on the cusp of deeply understanding language”. I think that sometimes we get so used to common knowledge that it looses some of its wonder. Why should distributional models perform well at analogies? Shouldn’t analogies be a generally difficult task which require deep understanding? Surely completing analogies requires at least as much semantic information (if not more) as assigning pronouns to their proper referent?\n\nOf course, the field of linguistics has recognized the somewhat counter intuitive result that in-fact the reverse is true. It is surprisingly hard to assign roles to pronouns, but word-ratings have been shown to be capable of completing analogies since the 1970’s! (source)\n\nWinograd Schemas\n\nThe Winograd Schema Challenge (WSC) is a test of machine intelligence that was proposed by Hector Levesque (UoT). Roughly, it is a pair of sentences which contain two noun phrases and an ambiguous pronoun which refers to different noun-phrases in the sentences. The prototypical example is given below:\n\n\n  The city councilmen refused the demonstrators a permit because they feared violence.\n\n\nIn asking who they refers to, it seems a rather simplistic task to assign it to the committee. However, by just changing the last words, we can change the role asignment\n\n\n  The city councilmen refused the demonstrators a permit because they advocated violence.\n\n\nThis demonstrates that what might seem a simple task which can simply be answered through syntactic regularities of language, actually requires understanding the semantics of the sentence.\n\nMotivation\n\nSo, why did I reference analogies and Winograd schemas? Well, it is motivated by the success of word rating models at solving analogies. My hunch is that\n\nLooking for Analogies\nFollowing this line of thought, I considered whether we could derive word meanings through the use of assimilies in literature. For example, the sentence\n\n\n  He was fast like a cheetah\n\n\nTells us a prototypical feature of cheetahs; namely, that they are fast! In general, I considered whether looking for phrases such as “like a” followed by a noun to be able to give insight to the prototypical features we have for different objects. To this end, I searched 16,000 free eBooks for “like a”\n\nstring = \"like a\"\nsentences = os.popen('grep -r -i -w -h \"'+ string + '\"').read().splitlines()\n\n\nThis command found 490,000 examples. So, just randomly clicking through them, what are the examples?\n\n\n  Make me a offer, Mister Beeler. I’d like a offer if you’d care to make one\n\n\nNot quite what I had hoped. The first example uses the work in a literal sense!\n\n\n  and Jean Claude would retreat like a disparaged puppy\n\n\nA puppy retreating? I think puppies are cute and innocent, hardly disparaging or retreating\n\n\n  spires that reached to the heaven like a cathedral of praise, bringing symmetry to its background\n\n\nCathedrals, reaching. Okay, maybe, but still, pretty hard.\n\n\n  tool that the creature Fur-nose carried, like a rock used for opening nuts\n\n\nOne thing that seems to be a trend is that it is not always clear what adjective is referencing the noun following “like a”. If we are to have any hope, we will require a syntatic depedency parser.\n\n\n  Together they watched Paul flitting ahead like a human dragonfly\n\n\nHere, what the noun should be is not very clear! We clearly know that it should be dragonfly, but that is because we recognize human as an adjective and not noun.\n\nWith everything above given, I picked some of the more random examples. There are many examples for which the assimily was clear. For fun, some are included here:\n\n\n  The apartment was furnished, but it didn’t look like a home\n\n\nThis tells that “look” is strongly associated with home! This is great!\n\n\n  We continued going until we arrived at an open place like a plateau with many chambers\n\n\n“As a”\n\nFor the other kind of simily, we find a similar number of raw occurences of “as a”. In looking at the examples, some are:\n\n\n  “I’m not sure,” she said. “I usually read them as a set, not as individual”\n\n\nHere we have a tricky assignment. Are sets (a rather abstract concept) usually visual (as would be indicated by read)?\n\n\n  Tenderness in a man is not viewed as a manly thing\n\n\nOooo, here he have a negation. Another tricky case I had not considered! Not to mention, how should “thing” be handled?\n\n\n  “You served as a vassal of the Queen in my childhood,” Headred said\n\n\nHere I have included a positive example. Vassals serve!\n\n\n  and blow up a transformer without so much as a scratch?” Dodger asked\n\n\nHow would we handle common phrases such as “without so much …”. A tricky problem indeed. On top of the examples given above, one final tantelizing example was:\n\n\n  as ever-present as a heartbeat\n\n\nI think that this does tell us something deep about how we represent the concept “heartbeat”. But how is it captured?\n\nThere is an interesting idea that we might learn about abstract concepts through the use of metaphors. Metaphores allow us to extend our embdied knowledge in useful ways. Maybe we can come up with a Reg-Ex expression to search for metaphores. This is i\n",
        "url": "/2020/02/20/assimilies/"
      },
    
      {
        "title": "Linear Dimensionality Reduction",
        "excerpt": "While there are many high-level descriptions of both Principle Component Analysis (PCA) and Independent Component Analysis (ICA), I thought I might write about a few key differences to solidify my own understanding. Both PCA and ICA are used as general dimensionality reduction techniques. However, the two methods operate under different assumptions and should not be confused with each other.\n\nCommon to Both\nWhat all decompositions seek to preserve is some distance between all pairs of points. Strictly speaking, this means that for the Euclidean metric, only rotations, flips, and translations are allowed (think all matrices with determinant=+-1). However, if we want to reduce the dimensionality of data, we necessarily need to discard some information. Most often, this corresponds to a linear projection into a subspace. Exactly how to determine which information should be “thrown away” depends on how we construct our projection matrix. This brings us to the two most common techniques\n\nPCA\nMost are familiar with PCA. Given some data that is multi-dimensional, the goal is to reduce the dimensionality of the data while still explaining as much of the variance as possible. In some sense, PCA can be thought of as fitting ellipses to data.\nCrucial to how PCA explains variance, one can visualize it as finding lines in a high dimensional space in which the data appear very spread out. This approach makes the most intuitive sense when one considers the underling variables of interest as uncorrelated gaussian random variables. This brings us to our first point:\n\nPCA performs source separation if the underlying variables are gaussian\n\nPoint 1:\nIf a signal is assumed to be a linear combination of independent gaussian sources along with gaussian noise, then PCA and ICA optimize the same objective function. This should make sense from what we have just described\n\nCorollary\n\n  The sum of two gaussian random variables is also gaussian!\n\n\nThis is important for understanding as it makes clear why PCA amounts to rotation of coordinates. In particular, PCA generates a subspace with orthogonal basis vectors which explain a maximal amount of variance.\n\nAssume X and Y are normally distributed variables\n\n\n\n\n\n\n\nthen:\n\n\n\nThis particular formula requires that the undying variables are uncorrelated, and independent, however these conditions can be relaxed and there is still a similar result.\n\nWhy PCA works\nPCA as matrix diagonalization. We know that matrices can be thought of as collections of vectors, and a vector can be thought of as a random variable. So to have two variables be uncorrelated means they have no covariance. But what is covariance? It is a dot product! That illustrates why it is always possible to generate a rotation which uncorrelates the variables!!\n\nSay we have N dimensions in our data (often # columns). Taking each column as a vector with u observations (# rows), we know that N independent vectors will at most spread an N dimensional space (even if there u » N observations).\n\nWe can take each column as a basis for this space, however, there is no reason to think the columns will be orthogonal. However, for any space, we can generate and orthonormal set of basis for the space! One way of doing this which is taught in many intro linear algebra courses is the gram-schmitt method.\n\nGram Schmitt\nWrite about the method here\n\nMore General\nThis orthognalization is normally done implicitly through SVD decomposition which generates unscaled principal components corresponding to the “left” eigen-vectors\n\nICA\nNow that we have good hold on PCA, we talk about ICA. First, and perhaps most importantly,Correlation is linear! Variables can be completely dependent (as in example 2), but uncorrelated! To summarize the discussion above, given two random variables, there is always a distance preserving transformation which uncorrelates the variables. Note, this is often called a whitening operation to make variables less co-linear. This is not the case for a different objective called Mutual Information (MI). For a more detailed discussion about what MI is, see my other post on the topic Mutual Information! Avoiding the details of how ICA is implemented, on a high level, a user apriori selects a target dimensionality, and a transfrom matrix is then created (often iteravly) which projects the original data onto a basis where the marginal distributions are as independent as possible (minimize MI of the target basis).\n\nWe can get a feel for what this means by making some examples.\n\nExample 1: Independent Gaussian Processes\nLet’s generate two independent gaussian variables:\n\nimport numpy as np\nA = .8*np.random.randn(1000);\nB = 3*np.random.randn(1000);\n\n\nWe can verify that these distributions are both uncorrelated and independent\n\nfrom sklearn import feature_selection as fs\nm = fs.mutual_info_regression(np.c_[A,B],B)\nm = m/max(m); r = np.corrcoef(A,B)\n\nThis gives the result of m=.00045 and r=.01. Now, We can correlate these signals by simply rotating the axis which generates a dependence. First, we will create a rotation matrix of 45 degrees, and apply this to our data. Here, a 2x2 matrix is multiplied (using @) by our row matrix\n\nimport seaborn as sns\nr = np.array([[np.cos(45), np.sin(45)],[-np.sin(45), np.cos(45)]]) @ np.c_[A,B].T\nsns.jointplot('A','B',data=pd.DataFrame({'A':r[0,:],'B':r[1,:]}))\n\n\n\nWe again calculate our correlation coefficient along with the mutual information!\n\nfrom sklearn import feature_selection as fs\nm = fs.mutual_info_regression(r.T,r[1,:].T)\nm = m/max(m); r = np.corrcoef(A,B)\n\nThis gives us the result of m=.11 and r=.84! Our intuition should be confirmed! For a more general case, we could consider the overall dependence of these metrics on angle of rotation.\n\nExample 2: Sensitivity to Rotation\nThe following code will generate a plot of our metrics based on angle of rotation.\n\nA = .8*np.random.randn(1000); B = 3*np.random.randn(1000);\ntheta=np.linspace(0,np.pi,100);\nvals = np.zeros((100,2))\nfor num in range(len(theta)):\n    r = np.array([[np.cos(theta[num]), np.sin(theta[num])],[-np.sin(theta[num]), np.cos(theta[num])]]) @ np.c_[A,B].T\n    vals[num,0]=np.corrcoef(r[0,:],r[1,:])[0,1]\n    m = fs.mutual_info_regression(r.T,r[1,:].T)\n    vals[num,1]=(m/max(m))[0]\nplt.plot(theta*180/3.14,vals); plt.grid(True)\n\n\n\nAs you might expect, correlations have a natural depedence on basis you choose to represent your data in! However, M.I. is a much more invariant measure to the particular choice of basis.\n\nExample 3: Dependent Uniform Processes\nThere are other sorts of distributions we can look at for comparing our two metrics. A common example used to demonstrate the difference between correlation and dependence is a modified uniform distribution. We can create two completely dependent variables from a uniform distribution using the abs() function\nX = np.random.uniform(-1,1,500);\nY = np.zeros(len(X)); Y[X&lt;0]=-X[X&lt;0]; Y[X&gt;0]=X[X&gt;0];\nsns.jointplot('X','Y',data=pd.DataFrame({'X':X,'Y':Y}),xlim=[-1,1])\n\n\n\nFor this plot, the M.I is 1, but the correlation is .02! Clearly, we see how limited the information correlation gives us is.\n\nExample 4: Spirals!!\nA fun example of data which from afar looks gaussian but up close isn’t are spirals! To get a general equation for a spiral, we can convert from polar coordinates. A line in polar coordinates:\n\n\n\n\n\nWe can then convert this to parametric coordinates and scale the axes seperatly\n\n\n\n\n\nThis now looks like an ellipsoid spiral. We can rotate any parametric equation by an angle  using the following equations:\n\n\n\n\n\nApplying this transformation we get the following plot\n\n\n\nWe can create these data points in python and look at the marginal distributions\ntheta=3.14/4\nt = np.arange(0,40,.2); \nx = .2*t*np.cos(t)\ny = .1*t*np.sin(t)\nu = x*np.cos(theta)-y*np.sin(theta);\nv = x*np.sin(theta)+y*np.cos(theta);\nsns.jointplot(u,v)\n\n\n\nAgain, we find that the correlation is .576 but the M.I is .02. This tells us that mutual information has missed the depedence. However, in the above discussion, we have glossed over a free parameter of the M.I. function in python: the number of nearest neighbors to consider. We can see the best that our function performs by iterating over all values for nearest neighbors.\n\nm=[]; ind=[];\nfor i in range(1,80):\n    ind.append(i)\n    tmp1 = fs.mutual_info_regression(u.reshape(-1,1),v,n_neighbors=i)\n    tmp2 = fs.mutual_info_regression(v.reshape(-1,1),v,n_neighbors=i)\n    m.append(tmp1/tmp2)\nplt.scatter(ind,m)\n\n\n\nWe see that there appears to be an upper limit. Another consideration are different symmetric shapes. If we change the parametric parameter to t=np.arrage(0,400,5) we get the below shape:\n\n\n\n\n\nThis shows us the difficulty in finding the proper number of nearest neighbors: For the first case, more neighbors was better. For the second case, it turned out 2 neighbors was optimal!\n\nClosing Thoughts\nIn the above discussion, we looked at some of the underlying differences between PCA and ICA with a focus on what mutual information looks like for some specific pairs of random variables. Like most things in research, I think the results emphasize how important it is to be familiar with your data to know what marginal distributions look like, and how that impacts the results of different dimensionality reduction techniques.\n\n",
        "content": "While there are many high-level descriptions of both Principle Component Analysis (PCA) and Independent Component Analysis (ICA), I thought I might write about a few key differences to solidify my own understanding. Both PCA and ICA are used as general dimensionality reduction techniques. However, the two methods operate under different assumptions and should not be confused with each other.\n\nCommon to Both\nWhat all decompositions seek to preserve is some distance between all pairs of points. Strictly speaking, this means that for the Euclidean metric, only rotations, flips, and translations are allowed (think all matrices with determinant=+-1). However, if we want to reduce the dimensionality of data, we necessarily need to discard some information. Most often, this corresponds to a linear projection into a subspace. Exactly how to determine which information should be “thrown away” depends on how we construct our projection matrix. This brings us to the two most common techniques\n\nPCA\nMost are familiar with PCA. Given some data that is multi-dimensional, the goal is to reduce the dimensionality of the data while still explaining as much of the variance as possible. In some sense, PCA can be thought of as fitting ellipses to data.\nCrucial to how PCA explains variance, one can visualize it as finding lines in a high dimensional space in which the data appear very spread out. This approach makes the most intuitive sense when one considers the underling variables of interest as uncorrelated gaussian random variables. This brings us to our first point:\n\nPCA performs source separation if the underlying variables are gaussian\n\nPoint 1:\nIf a signal is assumed to be a linear combination of independent gaussian sources along with gaussian noise, then PCA and ICA optimize the same objective function. This should make sense from what we have just described\n\nCorollary\n\n  The sum of two gaussian random variables is also gaussian!\n\n\nThis is important for understanding as it makes clear why PCA amounts to rotation of coordinates. In particular, PCA generates a subspace with orthogonal basis vectors which explain a maximal amount of variance.\n\nAssume X and Y are normally distributed variables\n\n\n\n\n\n\n\nthen:\n\n\n\nThis particular formula requires that the undying variables are uncorrelated, and independent, however these conditions can be relaxed and there is still a similar result.\n\nWhy PCA works\nPCA as matrix diagonalization. We know that matrices can be thought of as collections of vectors, and a vector can be thought of as a random variable. So to have two variables be uncorrelated means they have no covariance. But what is covariance? It is a dot product! That illustrates why it is always possible to generate a rotation which uncorrelates the variables!!\n\nSay we have N dimensions in our data (often # columns). Taking each column as a vector with u observations (# rows), we know that N independent vectors will at most spread an N dimensional space (even if there u » N observations).\n\nWe can take each column as a basis for this space, however, there is no reason to think the columns will be orthogonal. However, for any space, we can generate and orthonormal set of basis for the space! One way of doing this which is taught in many intro linear algebra courses is the gram-schmitt method.\n\nGram Schmitt\nWrite about the method here\n\nMore General\nThis orthognalization is normally done implicitly through SVD decomposition which generates unscaled principal components corresponding to the “left” eigen-vectors\n\nICA\nNow that we have good hold on PCA, we talk about ICA. First, and perhaps most importantly,Correlation is linear! Variables can be completely dependent (as in example 2), but uncorrelated! To summarize the discussion above, given two random variables, there is always a distance preserving transformation which uncorrelates the variables. Note, this is often called a whitening operation to make variables less co-linear. This is not the case for a different objective called Mutual Information (MI). For a more detailed discussion about what MI is, see my other post on the topic Mutual Information! Avoiding the details of how ICA is implemented, on a high level, a user apriori selects a target dimensionality, and a transfrom matrix is then created (often iteravly) which projects the original data onto a basis where the marginal distributions are as independent as possible (minimize MI of the target basis).\n\nWe can get a feel for what this means by making some examples.\n\nExample 1: Independent Gaussian Processes\nLet’s generate two independent gaussian variables:\n\nimport numpy as np\nA = .8*np.random.randn(1000);\nB = 3*np.random.randn(1000);\n\n\nWe can verify that these distributions are both uncorrelated and independent\n\nfrom sklearn import feature_selection as fs\nm = fs.mutual_info_regression(np.c_[A,B],B)\nm = m/max(m); r = np.corrcoef(A,B)\n\nThis gives the result of m=.00045 and r=.01. Now, We can correlate these signals by simply rotating the axis which generates a dependence. First, we will create a rotation matrix of 45 degrees, and apply this to our data. Here, a 2x2 matrix is multiplied (using @) by our row matrix\n\nimport seaborn as sns\nr = np.array([[np.cos(45), np.sin(45)],[-np.sin(45), np.cos(45)]]) @ np.c_[A,B].T\nsns.jointplot('A','B',data=pd.DataFrame({'A':r[0,:],'B':r[1,:]}))\n\n\n\nWe again calculate our correlation coefficient along with the mutual information!\n\nfrom sklearn import feature_selection as fs\nm = fs.mutual_info_regression(r.T,r[1,:].T)\nm = m/max(m); r = np.corrcoef(A,B)\n\nThis gives us the result of m=.11 and r=.84! Our intuition should be confirmed! For a more general case, we could consider the overall dependence of these metrics on angle of rotation.\n\nExample 2: Sensitivity to Rotation\nThe following code will generate a plot of our metrics based on angle of rotation.\n\nA = .8*np.random.randn(1000); B = 3*np.random.randn(1000);\ntheta=np.linspace(0,np.pi,100);\nvals = np.zeros((100,2))\nfor num in range(len(theta)):\n    r = np.array([[np.cos(theta[num]), np.sin(theta[num])],[-np.sin(theta[num]), np.cos(theta[num])]]) @ np.c_[A,B].T\n    vals[num,0]=np.corrcoef(r[0,:],r[1,:])[0,1]\n    m = fs.mutual_info_regression(r.T,r[1,:].T)\n    vals[num,1]=(m/max(m))[0]\nplt.plot(theta*180/3.14,vals); plt.grid(True)\n\n\n\nAs you might expect, correlations have a natural depedence on basis you choose to represent your data in! However, M.I. is a much more invariant measure to the particular choice of basis.\n\nExample 3: Dependent Uniform Processes\nThere are other sorts of distributions we can look at for comparing our two metrics. A common example used to demonstrate the difference between correlation and dependence is a modified uniform distribution. We can create two completely dependent variables from a uniform distribution using the abs() function\nX = np.random.uniform(-1,1,500);\nY = np.zeros(len(X)); Y[X&lt;0]=-X[X&lt;0]; Y[X&gt;0]=X[X&gt;0];\nsns.jointplot('X','Y',data=pd.DataFrame({'X':X,'Y':Y}),xlim=[-1,1])\n\n\n\nFor this plot, the M.I is 1, but the correlation is .02! Clearly, we see how limited the information correlation gives us is.\n\nExample 4: Spirals!!\nA fun example of data which from afar looks gaussian but up close isn’t are spirals! To get a general equation for a spiral, we can convert from polar coordinates. A line in polar coordinates:\n\n\n\n\n\nWe can then convert this to parametric coordinates and scale the axes seperatly\n\n\n\n\n\nThis now looks like an ellipsoid spiral. We can rotate any parametric equation by an angle  using the following equations:\n\n\n\n\n\nApplying this transformation we get the following plot\n\n\n\nWe can create these data points in python and look at the marginal distributions\ntheta=3.14/4\nt = np.arange(0,40,.2); \nx = .2*t*np.cos(t)\ny = .1*t*np.sin(t)\nu = x*np.cos(theta)-y*np.sin(theta);\nv = x*np.sin(theta)+y*np.cos(theta);\nsns.jointplot(u,v)\n\n\n\nAgain, we find that the correlation is .576 but the M.I is .02. This tells us that mutual information has missed the depedence. However, in the above discussion, we have glossed over a free parameter of the M.I. function in python: the number of nearest neighbors to consider. We can see the best that our function performs by iterating over all values for nearest neighbors.\n\nm=[]; ind=[];\nfor i in range(1,80):\n    ind.append(i)\n    tmp1 = fs.mutual_info_regression(u.reshape(-1,1),v,n_neighbors=i)\n    tmp2 = fs.mutual_info_regression(v.reshape(-1,1),v,n_neighbors=i)\n    m.append(tmp1/tmp2)\nplt.scatter(ind,m)\n\n\n\nWe see that there appears to be an upper limit. Another consideration are different symmetric shapes. If we change the parametric parameter to t=np.arrage(0,400,5) we get the below shape:\n\n\n\n\n\nThis shows us the difficulty in finding the proper number of nearest neighbors: For the first case, more neighbors was better. For the second case, it turned out 2 neighbors was optimal!\n\nClosing Thoughts\nIn the above discussion, we looked at some of the underlying differences between PCA and ICA with a focus on what mutual information looks like for some specific pairs of random variables. Like most things in research, I think the results emphasize how important it is to be familiar with your data to know what marginal distributions look like, and how that impacts the results of different dimensionality reduction techniques.\n\n",
        "url": "/2020/03/28/linear_dim_red/"
      },
    
      {
        "title": "Reveal, Markdown, and Jekyll",
        "excerpt": "\n\n## Overview\nThis is made to demonstrate how to make an online slideshow using only markdown\n\n---\n\n## Slide two\nI wanted to put in some fragments, so here is a list:\n* item 1 \n* item 2  \n\n---\n\n## Slide three\nThis is a code block\n```python\nimport numpy as np\nnp.load('file.npz')\n```\n\n---\n\n\n## Different Color\n\n\n\n",
        "content": "\n\n## Overview\nThis is made to demonstrate how to make an online slideshow using only markdown\n\n---\n\n## Slide two\nI wanted to put in some fragments, so here is a list:\n* item 1 \n* item 2  \n\n---\n\n## Slide three\nThis is a code block\n```python\nimport numpy as np\nnp.load('file.npz')\n```\n\n---\n\n\n## Different Color\n\n\n\n",
        "url": "/2020/04/20/demo/"
      },
    
  
  
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "\n",
    "url": "/categories/"
  },
  
  {
    "title": "Elements",
    "excerpt": "A demo of Markdown and HTML includes\n",
    "content": "Heading 1\n\nHeading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6\n\nA small element\n\nA link\n\nLorem ipsum dolor sit amet, consectetur adip* isicing elit, sed do eiusmod *tempor incididunt ut labore et dolore magna aliqua.\n\nDuis aute irure dolor in A link reprehenderit in voluptate velit esse cillum bold text dolore eu fugiat nulla pariatur. Excepteur span element sint occaecat cupidatat non proident, sunt italicised text in culpa qui officia deserunt mollit anim id some code est laborum.\n\n\n  An item\n  An item\n  An item\n  An item\n  An item\n\n\n\n  Item one\n  Item two\n  Item three\n  Item four\n  Item five\n\n\n\n  A simple blockquote\n\n\nSome HTML…\n\n&lt;blockquote cite=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;\n  &lt;p&gt;You planning a vacation, Mr. Sullivan?&lt;/p&gt;\n  &lt;footer&gt;\n    &lt;a href=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;Sunways Security Guard&lt;/a&gt;\n  &lt;/footer&gt;\n&lt;/blockquote&gt;\n\n\n…CSS…\n\nblockquote {\n  text-align: center;\n  font-weight: bold;\n}\nblockquote footer {\n  font-size: .8rem;\n}\n\n\n…and JavaScript\n\nconst blockquote = document.querySelector(\"blockquote\")\nconst bolden = (keyString, string) =&gt;\n  string.replace(new RegExp(keyString, 'g'), '&lt;strong&gt;'+keyString+'&lt;/strong&gt;')\n\nblockquote.innerHTML = bolden(\"Mr. Sullivan\", blockquote.innerHTML)\n\n\nSingle line of code\n\nHTML Includes\n\nContact form\n\n\n  \n    Contact\n    Name: *\n    \n\n    Email Address: *\n    \n\n    Message: *\n    \n\n    \n    \n    * indicates a required field\n\n    \n      \n      \n      \n    \n  \n\n\n\n\nPlease enable JavaScript to use the form.\n\n{% include site-form.html %}\n\n\nDemo map embed\n\n\n\n{% include map.html id=\"XXXXXX\" title=\"Coffee shop map\" %}\n\n\nButton include\n\nA button\n\nA button with icon  twitter\n\n\n{% include button.html text=\"A button\" link=\"https://david.darn.es\" %}\n{% include button.html text=\"A button with icon\" link=\"https://twitter.com/daviddarnes\" icon=\"twitter\" %}\n\n\nIcon include\n\ntwitter\ntwitter\n\n\n{% include icon.html id=\"twitter\" title=\"twitter\" %}\n[{% include icon.html id=\"linkedin\" title=\"twitter\" %}](https://www.linkedin.com/in/daviddarnes)\n\n\nVideo include\n\n\n  \n\n\n{% include video.html id=\"zrkcGL5H3MU\" title=\"Siteleaf tutorial video\" %}\n\n\nImage includes\n\n\n  \n  Image with caption\n\n\n\n  \n  Right aligned image\n\n\n\n  \n  Left aligned image\n\n\n\n  \n  \n\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Image with caption\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Right aligned image\" position=\"right\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Left aligned image\" position=\"left\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/1600/800?image=894\" alt=\"Image with just alt text\" %}\n\n",
    "url": "/elements/"
  },
  
  {
    "title": "Noether's Nonsense",
    "excerpt": "Alembic is a starting point for Jekyll projects. Rather than starting from scratch, this boilerplate is designed to get the ball rolling immediately. Install it, configure it, tweak it, push it.\n",
    "content": "Alembic is a starting point for Jekyll projects. Rather than starting from scratch, this boilerplate is designed to get the ball rolling immediately. Install it, configure it, tweak it, push it.\n\nFork it  github\n\n Tweet it  twitter\n\n Install Alembic ⚗️\n Tip me $5 💸\n\nFeatures\n\n\n  Available as a theme gem and GitHub Pages theme\n  Simple and elegant design that can be used out of the box or as solid starting point\n  Tested in all major browsers, including IE and Edge\n  Built in Service Worker so it can work offline and on slow connections\n  Configurable colours and typography in a single settings file\n  Extensive set of shortcodes to include various elements; such as buttons, icons, figure images and more\n  Solid typographic framework from Sassline\n  Configurable navigation via a single file\n  Modular Jekyll components\n  Post category support in the form of a single post index page grouped by category\n  Built in live search using JavaScript\n  Contact form built in using Formspree\n  Designed with Siteleaf in mind\n  Has 9 of the most popular networks as performant sharing buttons\n  Has documentation\n\n\nExamples\n\nHere are a few examples of Alembic out in the wild being used in a variety of ways:\n\n\n  bitpodcast.com\n  joelcagedesign.com\n  bawejakunal.github.io\n  case2111.github.io\n  www.10people.co.uk\n  hrkeni.me\n  ccs17.bsc.es\n  karateca.org\n\n\nInstallation\n\nQuick setup\n\nTo give you a running start I’ve put together some starter kits that you can download, fork or even deploy immediately:\n\n\n  ⚗️🍨 Vanilla Jekyll starter kit\n\n  ⚗️🌲 Forestry starter kit\n\n\n  \n    ⚗️💠 Netlify CMS starter kit\n\n  \n  \n    ⚗️:octocat: GitHub Pages with remote theme kit\nDownload kit\n  \n  ⚗️🚀 Stackbit starter kit\n\n\n\nAs a Jekyll theme\n\n\n  Add gem \"alembic-jekyll-theme\" to your Gemfile to add the theme as a dependancy\n  Run the command bundle install in the root of project to install the theme and its dependancies\n  Add theme: alembic-jekyll-theme to your _config.yml file to set the site theme\n  Run bundle exec jekyll serve to build and serve your site\n  Done! Use the configuration documentation and the example _config.yml file to set things like the navigation, contact form and social sharing buttons\n\n\nAs a GitHub Pages remote theme\n\n\n  Add gem \"jekyll-remote-theme\" to your Gemfile to add the theme as a dependancy\n  Run the command bundle install in the root of project to install the jekyll remote theme gem as a dependancy\n  Add jekyll-remote-theme to the list of plugins in your _config.yml file\n  Add remote_theme: daviddarnes/alembic to your _config.yml file to set the site theme\n  Run bundle exec jekyll serve to build and serve your site\n  Done! Use the configuration documentation and the example _config.yml file to set things like the navigation, contact form and social sharing buttons\n\n\nAs a Boilerplate / Fork\n\n(deprecated, not recommended)\n\n\n  Fork the repo\n  Replace the Gemfile with one stating all the gems used in your project\n  Delete the following unnecessary files/folders: .github, LICENSE, screenshot.png, CNAME and alembic-jekyll-theme.gemspec\n  Run the command bundle install in the root of project to install the jekyll remote theme gem as a dependancy\n  Run bundle exec jekyll serve to build and serve your site\n  Done! Use the configuration documentation and the example _config.yml file to set things like the navigation, contact form and social sharing buttons\n\n\nCustomising\n\nWhen using Alembic as a theme means you can take advantage of the file overriding method. This allows you to overwrite any file in this theme with your own custom file, simply by matching the file name and path. The most common example of this would be if you want to add your own styles or change the core style settings.\n\nTo add your own styles copy the styles.scss into your own project with the same file path (assets/styles.scss). From there you can add your own styles, you can even optionally ignore the theme styles by removing the @import \"alembic\"; line.\n\nIf you’re looking to set your own colours and fonts you can overwrite them by matching the variable names from the _settings.scss file in your own styles.scss, make sure to state them before the @import \"alembic\"; line so they take effect. The settings are a mixture of custom variables and settings from Sassline - follow the link to find out how to configure the typographic settings.\n",
    "url": "/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

